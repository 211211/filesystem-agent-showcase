"""
Search results cache with scope-aware invalidation.

This module provides a specialized cache for search operation results (grep, find, etc.)
with intelligent invalidation based on file system changes. The cache uses deterministic
key generation and tracks file state to automatically invalidate stale results.
"""

import hashlib
import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple

from app.cache.disk_cache import PersistentCache
from app.cache.file_state import FileState, FileStateTracker
from app.interfaces.cache import ISearchCache

logger = logging.getLogger(__name__)

# Maximum number of files to track per scope to avoid performance issues
MAX_TRACKED_FILES = 500


@dataclass
class ScopedSearchResult:
    """
    Cached search result with file state tracking.

    Stores both the search result and the state of all files in the search scope
    at the time the result was cached. This enables accurate cache invalidation
    when any file in the scope changes.

    Attributes:
        result: The search result string
        file_states: Dictionary mapping file paths to their state tuples (mtime, size, hash)
    """

    result: str
    file_states: Dict[str, Tuple[float, int, Optional[str]]]


class SearchCache(ISearchCache):
    """
    Cache search results with scope-aware invalidation.

    Implements the ISearchCache interface.

    This class provides caching for search operations (grep, find, etc.) with automatic
    invalidation when files in the search scope change. Cache keys are deterministically
    generated from the operation parameters, ensuring consistent lookups.

    Attributes:
        _cache: Underlying PersistentCache for storage
        _tracker: FileStateTracker for detecting file changes
        _search_prefix: Prefix for all search cache keys

    Example:
        >>> cache = PersistentCache()
        >>> tracker = FileStateTracker(cache)
        >>> search_cache = SearchCache(cache, tracker)
        >>>
        >>> # Cache a grep result
        >>> scope = Path("/path/to/search")
        >>> options = {"case_sensitive": True, "max_results": 100}
        >>> await search_cache.set_search_result(
        ...     operation="grep",
        ...     pattern="TODO",
        ...     scope=scope,
        ...     options=options,
        ...     result="file1.py:10:# TODO: fix this",
        ...     ttl=300
        ... )
        >>>
        >>> # Retrieve cached result (returns None if files changed)
        >>> result = await search_cache.get_search_result(
        ...     operation="grep",
        ...     pattern="TODO",
        ...     scope=scope,
        ...     options=options
        ... )
    """

    def __init__(
        self,
        disk_cache: PersistentCache,
        state_tracker: FileStateTracker,
        default_ttl: float = 300,
    ):
        """
        Initialize the search cache.

        Args:
            disk_cache: PersistentCache instance for storing search results
            state_tracker: FileStateTracker for detecting file changes
            default_ttl: Default time-to-live in seconds for cache entries (default: 300)
        """
        self._cache = disk_cache
        self._tracker = state_tracker
        self._search_prefix = "_search:"
        self._default_ttl = default_ttl

    def _make_key(
        self,
        operation: str,
        pattern: str,
        scope: Path,
        options: dict,
    ) -> str:
        """
        Create deterministic cache key for a search operation.

        The key is generated by hashing a JSON representation of the operation
        parameters, ensuring that identical searches produce identical keys.

        Args:
            operation: Type of search operation (e.g., "grep", "find")
            pattern: Search pattern or query
            scope: Directory or file path being searched
            options: Additional search options (e.g., case_sensitive, max_results)

        Returns:
            A deterministic cache key string prefixed with "_search:"

        Note:
            The key uses SHA256 hash (first 16 characters) of the sorted
            JSON representation, ensuring determinism across runs.

        Example:
            >>> key = cache._make_key(
            ...     operation="grep",
            ...     pattern="TODO",
            ...     scope=Path("/home/user/project"),
            ...     options={"case_sensitive": True}
            ... )
            >>> print(key)
            '_search:a1b2c3d4e5f6g7h8'
        """
        key_data = {
            "op": operation,
            "pattern": pattern,
            "scope": str(scope.resolve()),
            "options": sorted(options.items()),
        }
        key_str = json.dumps(key_data, sort_keys=True)
        key_hash = hashlib.sha256(key_str.encode()).hexdigest()[:16]
        return f"{self._search_prefix}{key_hash}"

    async def get_search_result(
        self,
        operation: str,
        pattern: str,
        scope: Path,
        options: dict,
    ) -> Optional[str]:
        """
        Get cached search result if scope hasn't changed.

        This method retrieves a cached search result and validates that the
        search scope (file or directory) hasn't been modified since the result
        was cached. If the scope is stale, the cache entry is deleted and None
        is returned.

        Args:
            operation: Type of search operation (e.g., "grep", "find")
            pattern: Search pattern or query
            scope: Directory or file path that was searched
            options: Search options used in the original search

        Returns:
            Cached search result string if valid, None if cache miss or scope is stale

        Example:
            >>> result = await search_cache.get_search_result(
            ...     operation="grep",
            ...     pattern="ERROR",
            ...     scope=Path("/var/log"),
            ...     options={"case_sensitive": False}
            ... )
            >>> if result:
            ...     print("Cache hit:", result)
            ... else:
            ...     print("Cache miss or stale")
        """
        key = self._make_key(operation, pattern, scope, options)

        # Get cached entry
        cached = await self._cache.get(key)
        if cached is None:
            logger.debug(
                f"Cache MISS: {operation} pattern={pattern} scope={scope}"
            )
            return None

        # Handle ScopedSearchResult with file state tracking
        if isinstance(cached, ScopedSearchResult):
            # Validate all tracked file states
            if await self._is_scope_stale_detailed(scope, cached.file_states):
                await self._cache.delete(key)
                logger.debug(
                    f"Cache MISS (stale - file changed): {operation} pattern={pattern} scope={scope}"
                )
                return None
            logger.debug(
                f"Cache HIT: {operation} pattern={pattern} scope={scope}"
            )
            return cached.result

        # Legacy cache entry without file tracking - treat as stale
        logger.debug(
            f"Cache MISS (legacy entry): {operation} pattern={pattern} scope={scope}"
        )
        await self._cache.delete(key)
        return None

    async def set_search_result(
        self,
        operation: str,
        pattern: str,
        scope: Path,
        options: dict,
        result: str,
        ttl: Optional[float] = None,
    ) -> None:
        """
        Cache search result with TTL and scope state tracking.

        Stores the search result in the cache with an expiration time and records
        the current state of all files in the search scope for future staleness detection.

        Args:
            operation: Type of search operation (e.g., "grep", "find")
            pattern: Search pattern or query
            scope: Directory or file path that was searched
            options: Search options used in the search
            result: Search result string to cache
            ttl: Time-to-live in seconds (default: uses self._default_ttl)

        Example:
            >>> await search_cache.set_search_result(
            ...     operation="find",
            ...     pattern="*.py",
            ...     scope=Path("/home/user/project"),
            ...     options={"recursive": True},
            ...     result="file1.py\\nfile2.py\\nfile3.py",
            ...     ttl=600  # 10 minutes
            ... )

        Note:
            The TTL ensures that even if files don't change, the cache entry
            will be refreshed periodically to account for potential missed changes.
        """
        key = self._make_key(operation, pattern, scope, options)

        # Collect file states for all files in scope
        file_states = await self._collect_file_states(scope)

        # Store as ScopedSearchResult
        entry = ScopedSearchResult(result=result, file_states=file_states)

        # Use effective TTL: provided ttl or default
        effective_ttl = ttl if ttl is not None else self._default_ttl
        await self._cache.set(key, entry, expire=effective_ttl)

        logger.debug(
            f"Cache SET: {operation} pattern={pattern} scope={scope} "
            f"files={len(file_states)} ttl={effective_ttl}s"
        )

    async def _is_scope_stale(self, scope: Path) -> bool:
        """
        Check if the search scope has changed since caching.

        Determines whether files in the search scope have been modified,
        added, or deleted since the search result was cached.

        Args:
            scope: Directory or file path to check

        Returns:
            True if the scope is stale (has changed), False otherwise

        Note:
            For files, checks the file's modification time, size, and content hash.
            For directories, checks the directory's modification time as a proxy
            for changes (faster than checking all files recursively).
        """
        if scope.is_file():
            return await self._tracker.is_stale(scope)

        # For directories, check directory mtime
        # (faster than checking all files)
        return await self._tracker.is_stale(scope)

    async def _update_scope_state(self, scope: Path) -> None:
        """
        Update the tracked state for the search scope.

        Records the current state (mtime, size, hash) of the scope for
        future staleness detection.

        Args:
            scope: Directory or file path to track
        """
        await self._tracker.update_state(scope)

    async def _collect_file_states(
        self, scope: Path
    ) -> Dict[str, Tuple[float, int, Optional[str]]]:
        """
        Collect current state of all files in scope.

        Gathers mtime, size, and content hash for each file in the search scope.
        For directories, recursively collects states for all contained files.

        Args:
            scope: Directory or file path to collect states from

        Returns:
            Dictionary mapping file paths to state tuples (mtime, size, content_hash)

        Note:
            - Limited to MAX_TRACKED_FILES (500) files to avoid performance issues
            - Logs a warning if scope has too many files
            - Inaccessible files are skipped with a debug log
        """
        file_states: Dict[str, Tuple[float, int, Optional[str]]] = {}

        if scope.is_file():
            try:
                state = FileState.from_path(scope, hash_content=True)
                file_states[str(scope.resolve())] = (
                    state.mtime,
                    state.size,
                    state.content_hash,
                )
            except (OSError, IOError) as e:
                logger.debug(f"Failed to collect state for {scope}: {e}")
            return file_states

        # Recursively collect file states for directories
        file_count = 0
        try:
            for file_path in scope.rglob("*"):
                if file_count >= MAX_TRACKED_FILES:
                    logger.warning(
                        f"Scope {scope} has more than {MAX_TRACKED_FILES} files. "
                        f"Tracking only the first {MAX_TRACKED_FILES} files. "
                        "Consider using a more specific search scope."
                    )
                    break

                if file_path.is_file():
                    try:
                        state = FileState.from_path(file_path, hash_content=True)
                        file_states[str(file_path.resolve())] = (
                            state.mtime,
                            state.size,
                            state.content_hash,
                        )
                        file_count += 1
                    except (OSError, IOError) as e:
                        logger.debug(f"Failed to collect state for {file_path}: {e}")
        except (OSError, IOError) as e:
            logger.debug(f"Failed to traverse scope {scope}: {e}")

        return file_states

    async def _is_scope_stale_detailed(
        self,
        scope: Path,
        cached_states: Dict[str, Tuple[float, int, Optional[str]]],
    ) -> bool:
        """
        Check if any file in scope has changed since caching.

        Compares the current state of files in the scope against cached states
        to detect additions, deletions, or modifications.

        Args:
            scope: Directory or file path to check
            cached_states: Dictionary of cached file states from ScopedSearchResult

        Returns:
            True if any file has changed (added, deleted, or modified), False otherwise
        """
        current_files: set[str] = set()

        if scope.is_file():
            current_files.add(str(scope.resolve()))
        else:
            try:
                file_count = 0
                for file_path in scope.rglob("*"):
                    if file_count >= MAX_TRACKED_FILES:
                        # If we hit the limit, check if cached also hit it
                        # If cached has fewer files, scope has grown - stale
                        if len(cached_states) < MAX_TRACKED_FILES:
                            return True
                        break

                    if file_path.is_file():
                        current_files.add(str(file_path.resolve()))
                        file_count += 1
            except (OSError, IOError) as e:
                logger.debug(f"Failed to traverse scope {scope}: {e}")
                return True  # Consider stale on error

        cached_files = set(cached_states.keys())

        # Check for added or deleted files
        if current_files != cached_files:
            logger.debug(
                f"Scope {scope} stale: file set changed. "
                f"Added: {current_files - cached_files}, "
                f"Deleted: {cached_files - current_files}"
            )
            return True

        # Check each file for content changes
        for file_path_str, (mtime, size, content_hash) in cached_states.items():
            file_path = Path(file_path_str)
            try:
                current_state = FileState.from_path(file_path, hash_content=True)
                if (
                    current_state.mtime != mtime
                    or current_state.size != size
                    or current_state.content_hash != content_hash
                ):
                    logger.debug(f"Scope {scope} stale: file {file_path} changed")
                    return True
            except FileNotFoundError:
                logger.debug(f"Scope {scope} stale: file {file_path} was deleted")
                return True
            except (OSError, IOError) as e:
                logger.debug(f"Failed to check state for {file_path}: {e}")
                return True  # Consider stale on error

        return False

    async def invalidate_pattern(
        self,
        operation: str,
        pattern: str,
        scope: Path,
        options: dict,
    ) -> bool:
        """
        Manually invalidate a specific search cache entry.

        Useful for forcing cache refresh for a specific search operation
        without waiting for automatic staleness detection.

        Args:
            operation: Type of search operation
            pattern: Search pattern
            scope: Directory or file path
            options: Search options

        Returns:
            True if the entry was found and deleted, False otherwise

        Example:
            >>> deleted = await search_cache.invalidate_pattern(
            ...     operation="grep",
            ...     pattern="TODO",
            ...     scope=Path("/home/user/project"),
            ...     options={"case_sensitive": True}
            ... )
            >>> if deleted:
            ...     print("Cache entry invalidated")
        """
        key = self._make_key(operation, pattern, scope, options)
        deleted = await self._cache.delete(key)

        if deleted:
            logger.info(
                f"Cache INVALIDATED: {operation} pattern={pattern} scope={scope}"
            )
        else:
            logger.debug(
                f"Cache INVALIDATE (not found): {operation} pattern={pattern} scope={scope}"
            )

        return deleted
