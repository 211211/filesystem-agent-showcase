# Ph√¢n T√≠ch Directed Acyclic Graph (DAG) Cho D·ª± √Ån Filesystem Agent

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan V·ªÅ DAG](#1-t·ªïng-quan-v·ªÅ-dag)
2. [DAG Trong H·ªá Th·ªëng AI Agent](#2-dag-trong-h·ªá-th·ªëng-ai-agent)
3. [Ph√¢n T√≠ch Ki·∫øn Tr√∫c Hi·ªán T·∫°i](#3-ph√¢n-t√≠ch-ki·∫øn-tr√∫c-hi·ªán-t·∫°i)
4. [C∆° H·ªôi √Åp D·ª•ng DAG](#4-c∆°-h·ªôi-√°p-d·ª•ng-dag)
5. [ƒê·ªÅ Xu·∫•t Ki·∫øn Tr√∫c DAG](#5-ƒë·ªÅ-xu·∫•t-ki·∫øn-tr√∫c-dag)
6. [Roadmap Tri·ªÉn Khai](#6-roadmap-tri·ªÉn-khai)
7. [So S√°nh V·ªõi LangGraph](#7-so-s√°nh-v·ªõi-langgraph)

---

## 1. T·ªïng Quan V·ªÅ DAG

### 1.1 ƒê·ªãnh Nghƒ©a

**Directed Acyclic Graph (ƒê·ªì Th·ªã C√≥ H∆∞·ªõng Kh√¥ng Chu Tr√¨nh)** l√† m·ªôt c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªì th·ªã v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm:

- **Directed (C√≥ h∆∞·ªõng)**: C√°c c·∫°nh c√≥ h∆∞·ªõng t·ª´ node n√†y sang node kh√°c (A ‚Üí B)
- **Acyclic (Kh√¥ng chu tr√¨nh)**: Kh√¥ng t·ªìn t·∫°i ƒë∆∞·ªùng ƒëi quay tr·ªü l·∫°i ch√≠nh n√≥
- **Graph (ƒê·ªì th·ªã)**: T·∫≠p h·ª£p c√°c nodes (ƒë·ªânh) v√† edges (c·∫°nh)

```
        A
       ‚Üô ‚Üò
      B   C
       ‚Üò ‚Üô
        D
```

### 1.2 ∆Øu ƒêi·ªÉm C·ªßa DAG

#### a) T√≠nh X√°c ƒê·ªãnh (Deterministic)
- Kh√¥ng c√≥ v√≤ng l·∫∑p v√¥ h·∫°n
- Lu√¥n c√≥ th·ªÉ s·∫Øp x·∫øp th·ª© t·ª± th·ª±c thi (topological sort)
- D·ªÖ debug v√† trace execution flow

#### b) T·ªëi ∆Øu H√≥a
- **Parallel Execution**: C√°c node ƒë·ªôc l·∫≠p c√≥ th·ªÉ ch·∫°y song song
- **Dependency Management**: Qu·∫£n l√Ω dependencies r√µ r√†ng
- **Caching**: Cache k·∫øt qu·∫£ c·ªßa t·ª´ng node, t√°i s·ª≠ d·ª•ng khi dependencies kh√¥ng ƒë·ªïi

#### c) Kh·∫£ NƒÉng M·ªü R·ªông
- D·ªÖ th√™m/x√≥a nodes m√† kh√¥ng ·∫£nh h∆∞·ªüng to√†n b·ªô h·ªá th·ªëng
- C√≥ th·ªÉ visualize workflow m·ªôt c√°ch tr·ª±c quan
- H·ªó tr·ª£ conditional branching v√† error handling

### 1.3 ·ª®ng D·ª•ng Th·ª±c T·∫ø

| Lƒ©nh V·ª±c | V√≠ D·ª• |
|----------|-------|
| **Data Engineering** | Apache Airflow, Luigi, Prefect - Qu·∫£n l√Ω data pipelines |
| **Build Systems** | Makefile, Gradle, Bazel - Qu·∫£n l√Ω build dependencies |
| **Blockchain** | Bitcoin, Ethereum - Transaction ordering |
| **AI/ML** | TensorFlow, PyTorch - Computational graphs |
| **Package Management** | npm, pip, apt - Dependency resolution |
| **CI/CD** | GitHub Actions, GitLab CI - Pipeline execution |

---

## 2. DAG Trong H·ªá Th·ªëng AI Agent

### 2.1 Xu H∆∞·ªõng NƒÉm 2025-2026

Theo nghi√™n c·ª©u m·ªõi nh·∫•t (2026), DAG ƒëang tr·ªü th√†nh **backbone** c·ªßa c√°c h·ªá th·ªëng Multi-Agent AI:

#### a) LangGraph (LangChain Ecosystem)
```python
from langgraph.graph import Graph

# Define workflow as DAG
workflow = Graph()
workflow.add_node("research", research_agent)
workflow.add_node("analyze", analysis_agent)
workflow.add_node("synthesize", synthesis_agent)

# Define edges (dependencies)
workflow.add_edge("research", "analyze")
workflow.add_edge("analyze", "synthesize")
workflow.set_entry_point("research")
```

**ƒê·∫∑c ƒëi·ªÉm:**
- H·ªó tr·ª£ **cycles** (kh√°c v·ªõi DAG thu·∫ßn t√∫y) ƒë·ªÉ t·∫°o agent-like behaviors
- Lowest latency trong c√°c agentic frameworks (benchmark 2026)
- Declarative architecture v·ªõi static tool assignments

#### b) DAGent Framework
```python
from dagent import DAGAgent, Task

# Define tasks as nodes
task1 = Task("search", search_tool)
task2 = Task("analyze", analyze_tool, dependencies=[task1])
task3 = Task("report", report_tool, dependencies=[task2])

# Create DAG
agent = DAGAgent([task1, task2, task3])
result = await agent.execute()
```

**ƒê·∫∑c ƒëi·ªÉm:**
- Pure DAG approach (kh√¥ng c√≥ cycles)
- Support parallel v√† conditional execution
- DAG visualization built-in

#### c) Adaptive Multi-Agent Systems (2025+)
- **Dynamic DAG Restructuring**: DAG t·ª± ƒë·ªông thay ƒë·ªïi d·ª±a tr√™n runtime information
- **Resilience**: X·ª≠ l√Ω unpredictable scenarios b·∫±ng c√°ch thay ƒë·ªïi graph structure
- **Real-time Optimization**: ƒêi·ªÅu ch·ªânh execution path d·ª±a tr√™n performance metrics

### 2.2 L·ª£i √çch C·ªßa DAG Cho AI Agents

#### a) Tr√°nh Infinite Loops
```
‚ùå Without DAG:
Agent ‚Üí Tool A ‚Üí Agent ‚Üí Tool A ‚Üí Agent ‚Üí Tool A ... (infinite loop)

‚úÖ With DAG:
Agent ‚Üí [Tool A ‚Üí Tool B ‚Üí Tool C] ‚Üí Final Response (max depth enforced)
```

#### b) Optimized Execution
```python
# Sequential (current approach)
result1 = await execute_tool("grep pattern1 file1")  # 2s
result2 = await execute_tool("grep pattern2 file2")  # 2s
result3 = await execute_tool("grep pattern3 file3")  # 2s
# Total: 6s

# DAG Parallel (proposed)
results = await execute_dag_parallel([
    Node("grep1", depends_on=[]),
    Node("grep2", depends_on=[]),
    Node("grep3", depends_on=[]),
])
# Total: 2s (3x faster)
```

#### c) Dependency Management
```
find_files ‚Üí grep_in_files ‚Üí count_matches ‚Üí generate_report
     ‚Üì
  cat_file
```

---

## 3. Ph√¢n T√≠ch Ki·∫øn Tr√∫c Hi·ªán T·∫°i

### 3.1 Agent Execution Flow

**File**: `app/agent/filesystem_agent.py`

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FilesystemAgent                         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  chat() method - Agent Loop (max 10 iterations):            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  1. User Message ‚Üí LLM                                       ‚îÇ
‚îÇ  2. LLM returns tool_calls                                   ‚îÇ
‚îÇ  3. Execute tools (sequential or parallel)                   ‚îÇ
‚îÇ  4. Feed results back to LLM                                 ‚îÇ
‚îÇ  5. Repeat until LLM returns final response                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ParallelToolOrchestrator                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  - analyze_dependencies(): Ph√¢n lo·∫°i READ vs WRITE tools    ‚îÇ
‚îÇ  - execute_parallel(): Ch·∫°y read-only tools ƒë·ªìng th·ªùi       ‚îÇ
‚îÇ  - execute_sequential(): Ch·∫°y write tools tu·∫ßn t·ª±           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   SandboxExecutor                            ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  - Whitelist commands (grep, find, cat, head, tail, ls, wc) ‚îÇ
‚îÇ  - Path confinement (security)                               ‚îÇ
‚îÇ  - Timeout protection                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Current Limitations

#### a) Limited Dependency Awareness
```python
# orchestrator.py:197
def analyze_dependencies(self, tool_calls):
    # Ch·ªâ ph√¢n bi·ªát READ vs WRITE
    # Kh√¥ng hi·ªÉu dependencies gi·ªØa c√°c tools
    if tc.name in READ_ONLY_TOOLS:
        read_only_calls.append(tc)
    elif tc.name in WRITE_TOOLS:
        write_calls.append(tc)
```

**V·∫•n ƒë·ªÅ:**
- Tool B c·∫ßn k·∫øt qu·∫£ c·ªßa Tool A nh∆∞ng v·∫´n ch·∫°y parallel
- Kh√¥ng t·ªëi ∆∞u cho workflows ph·ª©c t·∫°p

**V√≠ d·ª•:**
```python
# Scenario: Find Python files then grep them
tool_calls = [
    ToolCall("find", {"path": ".", "name_pattern": "*.py"}),
    ToolCall("grep", {"pattern": "import", "path": "result_from_find"})  # ‚ùå Depends on find!
]
# Hi·ªán t·∫°i: Ch·∫°y song song ‚Üí grep fails v√¨ ch∆∞a c√≥ result
# L√Ω t∆∞·ªüng: Ch·∫°y find tr∆∞·ªõc, sau ƒë√≥ grep
```

#### b) No Built-in Workflow Visualization
- Kh√≥ debug khi c√≥ nhi·ªÅu tool calls
- Kh√¥ng th·ªÉ visualize execution flow
- Hard to explain to users what the agent is doing

#### c) Fixed Iteration Limit
```python
# filesystem_agent.py:466
for iteration in range(self.max_tool_iterations):  # max 10 iterations
```

**V·∫•n ƒë·ªÅ:**
- C√≥ th·ªÉ d·ª´ng gi·ªØa ch·ª´ng n·∫øu workflow ph·ª©c t·∫°p
- Kh√¥ng c√≥ dynamic adjustment based on complexity

### 3.3 Existing Parallelization

**File**: `app/agent/orchestrator.py`

```python
class ParallelToolOrchestrator:
    async def execute_parallel(self, tool_calls):
        """Execute multiple tool calls in parallel using asyncio.gather."""
        tasks = [self.execute_tool_with_semaphore(tc) for tc in tool_calls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
```

**∆Øu ƒëi·ªÉm hi·ªán t·∫°i:**
- ‚úÖ Semaphore limiting (max_concurrent=5)
- ‚úÖ Exception handling
- ‚úÖ Read-only tool detection

**H·∫°n ch·∫ø:**
- ‚ùå No fine-grained dependency tracking
- ‚ùå All-or-nothing parallelization (either all parallel or all sequential)
- ‚ùå No partial ordering

---

## 4. C∆° H·ªôi √Åp D·ª•ng DAG

### 4.1 Use Case 1: Tool Execution DAG

#### V·∫•n ƒê·ªÅ Hi·ªán T·∫°i
```python
# Agent receives complex query:
"Find all Python files, count lines in each, and list files with > 100 lines"

# LLM generates tool calls:
[
    find(".", "*.py"),        # Must run first
    cat("file1.py"),          # Depends on find result
    wc("file1.py"),           # Depends on find result
    cat("file2.py"),          # Depends on find result
    wc("file2.py"),           # Depends on find result
    grep(">100", "wc_results") # Depends on all wc results
]

# Current execution: Sequential or all-parallel (not optimal)
```

#### Gi·∫£i Ph√°p DAG
```
                   find(*.py)
                       ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì              ‚Üì               ‚Üì
    wc(file1)      wc(file2)      wc(file3)    ‚Üê Parallel layer
        ‚Üì              ‚Üì               ‚Üì
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
                 aggregate_results            ‚Üê Final node
```

**Performance gain:**
- Sequential: `T_find + 3*(T_wc) = 0.5s + 3*1s = 3.5s`
- DAG: `T_find + max(T_wc) = 0.5s + 1s = 1.5s` **(2.3x faster)**

### 4.2 Use Case 2: Cache Invalidation DAG

#### V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

**File**: `app/cache/cache_manager.py`

```python
# When file changes:
# 1. ContentCache invalidation
# 2. SearchCache invalidation (if file is in scope)
# 3. FileStateTracker update

# But: No explicit dependency graph!
```

**V·∫•n ƒë·ªÅ:**
- Khi `data/project/module.py` thay ƒë·ªïi:
  - Invalidate content cache cho `module.py` ‚úì
  - Invalidate search cache cho `grep "import" data/project/` ‚úì
  - Nh∆∞ng c√≥ c·∫ßn invalidate `grep "import" data/`? ‚Üí **Dependency graph unclear**

#### Gi·∫£i Ph√°p DAG

```python
# Cache Invalidation DAG
class CacheInvalidationDAG:
    """
    Track cache dependencies as a DAG.

    Example:
        search_cache("grep import data/")
            ‚Üì depends_on
        [content("data/a.py"), content("data/b.py"), content("data/sub/c.py")]
            ‚Üì depends_on
        [file_state("data/a.py"), file_state("data/b.py"), ...]
    """
```

**Khi file thay ƒë·ªïi:**
```
file_state("data/a.py") changed
    ‚Üì invalidates
content_cache("data/a.py")
    ‚Üì invalidates
search_cache("grep import data/")
    ‚Üì invalidates
search_cache("grep import data/project/")
```

**Benefits:**
- **Precise invalidation**: Ch·ªâ invalidate nh·ªØng g√¨ c·∫ßn thi·∫øt
- **Dependency tracking**: Bi·∫øt search n√†o ph·ª• thu·ªôc v√†o file n√†o
- **Performance**: Tr√°nh over-invalidation

### 4.3 Use Case 3: Multi-Stage Agent Workflow

#### Scenario: Complex Analysis Task

```
User: "Analyze this codebase: find all TODO comments, categorize by severity,
       and generate a priority report"
```

**Traditional approach (current):**
```python
# 10 iterations of LLM back-and-forth
iter1: LLM decides to grep "TODO"
iter2: LLM sees results, decides to categorize
iter3: LLM asks for more context
...
iter10: Finally generates report (or hits limit)
```

**DAG approach:**
```
                    Analyze Request
                          ‚Üì
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚Üì                   ‚Üì
        Find TODOs            Extract Context
           (grep)                (cat files)
                ‚Üì                   ‚Üì
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
                  Categorize by Pattern
                    (regex + LLM)
                          ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚Üì           ‚Üì
            High Priority   Low Priority
                    ‚Üì           ‚Üì
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
                  Generate Report
```

**Benefits:**
- **Fewer LLM calls**: 3-4 instead of 10 (cost reduction)
- **Faster execution**: Parallel branches
- **More predictable**: Pre-defined workflow structure

### 4.4 Use Case 4: Dynamic Tool Planning

#### Concept: LLM-Generated DAG

```python
class DynamicDAGPlanner:
    """
    Ask LLM to generate a DAG plan before executing tools.

    Flow:
    1. User query ‚Üí LLM
    2. LLM generates DAG plan (JSON)
    3. Validate DAG (acyclic, tools exist)
    4. Execute DAG with optimal parallelization
    5. Return results
    """

    async def plan(self, query: str) -> DAG:
        response = await self.llm.chat(f"""
        Create an execution plan as a DAG for this query: {query}

        Return JSON:
        {{
            "nodes": [
                {{"id": "1", "tool": "find", "args": {{"pattern": "*.py"}}}},
                {{"id": "2", "tool": "grep", "args": {{"pattern": "TODO"}}, "depends_on": ["1"]}}
            ]
        }}
        """)
        return DAG.from_json(response)
```

**Example:**

```json
{
  "query": "Find large Python files and analyze imports",
  "dag": {
    "nodes": [
      {
        "id": "find_files",
        "tool": "find",
        "args": {"path": ".", "name_pattern": "*.py"}
      },
      {
        "id": "check_size",
        "tool": "wc",
        "args": {"files": "$find_files.output"},
        "depends_on": ["find_files"]
      },
      {
        "id": "filter_large",
        "tool": "filter",
        "args": {"threshold": 100},
        "depends_on": ["check_size"]
      },
      {
        "id": "analyze_imports",
        "tool": "grep",
        "args": {"pattern": "^import|^from", "files": "$filter_large.output"},
        "depends_on": ["filter_large"]
      }
    ]
  }
}
```

---

## 5. ƒê·ªÅ Xu·∫•t Ki·∫øn Tr√∫c DAG

### 5.1 Core Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FilesystemAgent                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  - chat(): Entry point                                          ‚îÇ
‚îÇ  - chat_stream(): Streaming variant                            ‚îÇ
‚îÇ  - get_cache_stats(): Monitoring                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       DAGPlanner (NEW)                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  - analyze_query(): Understand user intent                     ‚îÇ
‚îÇ  - generate_dag(): Create execution plan as DAG               ‚îÇ
‚îÇ  - optimize_dag(): Apply optimizations (merge, prune)         ‚îÇ
‚îÇ  - validate_dag(): Check for cycles, invalid tools            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       DAGExecutor (NEW)                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  - topological_sort(): Order nodes for execution              ‚îÇ
‚îÇ  - execute_dag(): Run DAG with optimal parallelization        ‚îÇ
‚îÇ  - handle_errors(): Error recovery and retries                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 ParallelToolOrchestrator (Enhanced)            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  - execute_layer(): Execute one layer of DAG in parallel      ‚îÇ
‚îÇ  - dependency_check(): Verify dependencies satisfied          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CacheManager (Enhanced)                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  - dag_aware_cache: Cache with DAG dependencies               ‚îÇ
‚îÇ  - invalidate_dag(): Propagate invalidation through DAG       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 DAG Data Structure

```python
# app/agent/dag.py
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any
from enum import Enum

class NodeStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class DAGNode:
    """
    A node in the execution DAG.

    Attributes:
        id: Unique identifier for the node
        tool_name: Name of the tool to execute (grep, find, cat, etc.)
        arguments: Arguments to pass to the tool
        depends_on: List of node IDs that must complete before this node
        status: Current execution status
        result: Execution result (populated after completion)
        error: Error information (if failed)
    """
    id: str
    tool_name: str
    arguments: Dict[str, Any]
    depends_on: List[str] = field(default_factory=list)
    status: NodeStatus = NodeStatus.PENDING
    result: Optional[Any] = None
    error: Optional[str] = None

    def is_ready(self, completed_nodes: set[str]) -> bool:
        """Check if all dependencies are satisfied."""
        return all(dep in completed_nodes for dep in self.depends_on)

    def can_run_parallel_with(self, other: "DAGNode") -> bool:
        """Check if this node can run in parallel with another node."""
        # Can't run parallel if there's a dependency relationship
        if other.id in self.depends_on or self.id in other.depends_on:
            return False

        # Can't run parallel if they share a dependency that modifies state
        # (current implementation assumes all tools are read-only)
        return True

@dataclass
class ExecutionDAG:
    """
    Directed Acyclic Graph for tool execution.

    Attributes:
        nodes: Dictionary of node_id -> DAGNode
        entry_points: Nodes with no dependencies (start here)
        exit_points: Nodes with no dependents (final results)
    """
    nodes: Dict[str, DAGNode] = field(default_factory=dict)
    entry_points: List[str] = field(default_factory=list)
    exit_points: List[str] = field(default_factory=list)

    def add_node(self, node: DAGNode) -> None:
        """Add a node to the DAG."""
        self.nodes[node.id] = node

        # Update entry/exit points
        if not node.depends_on:
            self.entry_points.append(node.id)

        # Update exit points (nodes with no dependents)
        self._update_exit_points()

    def add_edge(self, from_id: str, to_id: str) -> None:
        """Add a dependency edge from one node to another."""
        if to_id not in self.nodes:
            raise ValueError(f"Node {to_id} not found")

        self.nodes[to_id].depends_on.append(from_id)
        self._update_exit_points()

    def validate(self) -> tuple[bool, Optional[str]]:
        """
        Validate the DAG structure.

        Returns:
            (is_valid, error_message)
        """
        # Check for cycles using DFS
        visited = set()
        rec_stack = set()

        def has_cycle(node_id: str) -> bool:
            visited.add(node_id)
            rec_stack.add(node_id)

            # Check all dependencies
            node = self.nodes[node_id]
            for dep_id in node.depends_on:
                if dep_id not in self.nodes:
                    return True  # Invalid dependency

                if dep_id not in visited:
                    if has_cycle(dep_id):
                        return True
                elif dep_id in rec_stack:
                    return True  # Cycle detected

            rec_stack.remove(node_id)
            return False

        for node_id in self.nodes:
            if node_id not in visited:
                if has_cycle(node_id):
                    return False, f"Cycle detected involving node {node_id}"

        return True, None

    def topological_sort(self) -> List[List[str]]:
        """
        Return nodes in topological order, grouped by execution layers.

        Nodes in the same layer can be executed in parallel.

        Returns:
            List of layers, where each layer is a list of node IDs

        Example:
            [
                ["find_files"],                    # Layer 0: entry points
                ["wc_file1", "wc_file2", "wc_file3"],  # Layer 1: parallel
                ["aggregate_results"]              # Layer 2: final
            ]
        """
        # Calculate in-degree for each node
        in_degree = {node_id: len(node.depends_on) for node_id, node in self.nodes.items()}

        layers = []
        completed = set()

        while len(completed) < len(self.nodes):
            # Find all nodes with in-degree 0 (ready to execute)
            current_layer = [
                node_id for node_id, degree in in_degree.items()
                if degree == 0 and node_id not in completed
            ]

            if not current_layer:
                # No nodes ready -> there must be a cycle (shouldn't happen after validation)
                raise ValueError("DAG has a cycle or is invalid")

            layers.append(current_layer)

            # Mark nodes as completed and update in-degrees
            for node_id in current_layer:
                completed.add(node_id)

                # Decrease in-degree for dependent nodes
                for other_id, other_node in self.nodes.items():
                    if node_id in other_node.depends_on:
                        in_degree[other_id] -= 1

        return layers

    def get_execution_plan(self) -> Dict[str, Any]:
        """
        Generate a human-readable execution plan.

        Returns:
            Dictionary with execution plan details
        """
        layers = self.topological_sort()

        return {
            "total_nodes": len(self.nodes),
            "total_layers": len(layers),
            "max_parallelism": max(len(layer) for layer in layers),
            "layers": [
                {
                    "layer_id": i,
                    "node_count": len(layer),
                    "nodes": [
                        {
                            "id": node_id,
                            "tool": self.nodes[node_id].tool_name,
                            "depends_on": self.nodes[node_id].depends_on,
                        }
                        for node_id in layer
                    ]
                }
                for i, layer in enumerate(layers)
            ]
        }

    def _update_exit_points(self) -> None:
        """Update the list of exit points (nodes with no dependents)."""
        dependents = set()
        for node in self.nodes.values():
            dependents.update(node.depends_on)

        self.exit_points = [
            node_id for node_id in self.nodes
            if node_id not in dependents
        ]

    def visualize(self) -> str:
        """
        Generate a text-based visualization of the DAG.

        Returns:
            ASCII art representation of the DAG
        """
        layers = self.topological_sort()

        lines = []
        lines.append("Execution DAG:")
        lines.append("=" * 60)

        for i, layer in enumerate(layers):
            lines.append(f"\nLayer {i} (parallel group):")
            for node_id in layer:
                node = self.nodes[node_id]
                deps = ", ".join(node.depends_on) if node.depends_on else "none"
                lines.append(f"  ‚Ä¢ {node_id}: {node.tool_name}()")
                lines.append(f"    depends_on: {deps}")

        return "\n".join(lines)
```

### 5.3 DAG Executor

```python
# app/agent/dag_executor.py
import asyncio
import logging
from typing import Dict, List, Optional
from app.agent.dag import ExecutionDAG, DAGNode, NodeStatus
from app.agent.filesystem_agent import ToolCall
from app.sandbox.executor import ExecutionResult

logger = logging.getLogger(__name__)

class DAGExecutor:
    """
    Executes a DAG of tool calls with optimal parallelization.

    This executor:
    1. Validates the DAG structure
    2. Performs topological sort to determine execution order
    3. Executes nodes layer-by-layer, parallelizing within each layer
    4. Handles errors and propagates results
    """

    def __init__(
        self,
        orchestrator,  # ParallelToolOrchestrator
        max_concurrent: int = 5,
    ):
        self.orchestrator = orchestrator
        self.max_concurrent = max_concurrent

    async def execute(self, dag: ExecutionDAG) -> Dict[str, ExecutionResult]:
        """
        Execute the DAG and return results.

        Args:
            dag: The ExecutionDAG to execute

        Returns:
            Dictionary mapping node_id to ExecutionResult

        Raises:
            ValueError: If DAG is invalid
        """
        # Validate DAG
        is_valid, error = dag.validate()
        if not is_valid:
            raise ValueError(f"Invalid DAG: {error}")

        logger.info(f"Executing DAG with {len(dag.nodes)} nodes")
        logger.debug(f"Execution plan:\n{dag.visualize()}")

        # Get execution layers
        layers = dag.topological_sort()
        logger.info(f"DAG has {len(layers)} execution layers")

        results: Dict[str, ExecutionResult] = {}

        # Execute layer by layer
        for layer_idx, layer_nodes in enumerate(layers):
            logger.info(f"Executing layer {layer_idx} with {len(layer_nodes)} nodes")

            # Execute all nodes in this layer in parallel
            layer_results = await self._execute_layer(dag, layer_nodes, results)
            results.update(layer_results)

            # Check for failures
            failed_nodes = [
                node_id for node_id in layer_nodes
                if not results[node_id].success
            ]

            if failed_nodes:
                logger.warning(f"Layer {layer_idx} had {len(failed_nodes)} failures")
                # Mark dependent nodes as skipped
                self._mark_dependents_skipped(dag, failed_nodes)

        logger.info(f"DAG execution completed. {len(results)} nodes executed.")
        return results

    async def _execute_layer(
        self,
        dag: ExecutionDAG,
        node_ids: List[str],
        previous_results: Dict[str, ExecutionResult],
    ) -> Dict[str, ExecutionResult]:
        """
        Execute a single layer of nodes in parallel.

        Args:
            dag: The execution DAG
            node_ids: List of node IDs to execute in this layer
            previous_results: Results from previously executed nodes

        Returns:
            Dictionary mapping node_id to ExecutionResult for this layer
        """
        # Create ToolCall objects for the orchestrator
        tool_calls = []
        node_map = {}  # Map ToolCall.id -> node_id

        for node_id in node_ids:
            node = dag.nodes[node_id]

            # Resolve arguments (substitute results from dependencies)
            resolved_args = self._resolve_arguments(node, previous_results)

            # Create ToolCall
            tool_call = ToolCall(
                id=node_id,  # Use node_id as tool_call_id
                name=node.tool_name,
                arguments=resolved_args,
            )
            tool_calls.append(tool_call)
            node_map[node_id] = node

        # Execute tools in parallel using orchestrator
        execution_results = await self.orchestrator.execute_parallel(tool_calls)

        # Map results back to nodes
        results = {}
        for tool_call, result in execution_results:
            node_id = tool_call.id
            node = node_map[node_id]

            # Update node status
            if result.success:
                node.status = NodeStatus.COMPLETED
                node.result = result
            else:
                node.status = NodeStatus.FAILED
                node.error = result.stderr

            results[node_id] = result

        return results

    def _resolve_arguments(
        self,
        node: DAGNode,
        previous_results: Dict[str, ExecutionResult],
    ) -> Dict[str, any]:
        """
        Resolve node arguments, substituting references to previous results.

        Example:
            node.arguments = {"files": "$find_files.output"}
            previous_results = {"find_files": ExecutionResult(stdout="a.py\nb.py")}

            returns: {"files": "a.py\nb.py"}
        """
        resolved = {}

        for key, value in node.arguments.items():
            if isinstance(value, str) and value.startswith("$"):
                # Reference to another node's output
                ref_node_id = value[1:].split(".")[0]

                if ref_node_id not in previous_results:
                    raise ValueError(f"Unresolved dependency: {ref_node_id}")

                # Get the output from the referenced node
                resolved[key] = previous_results[ref_node_id].stdout
            else:
                resolved[key] = value

        return resolved

    def _mark_dependents_skipped(
        self,
        dag: ExecutionDAG,
        failed_nodes: List[str],
    ) -> None:
        """
        Mark all nodes dependent on failed nodes as SKIPPED.

        This prevents wasting time executing nodes that depend on failed operations.
        """
        to_skip = set(failed_nodes)

        # Iteratively find all dependents
        changed = True
        while changed:
            changed = False
            for node_id, node in dag.nodes.items():
                if node.status == NodeStatus.PENDING:
                    if any(dep in to_skip for dep in node.depends_on):
                        node.status = NodeStatus.SKIPPED
                        to_skip.add(node_id)
                        changed = True

        if len(to_skip) > len(failed_nodes):
            logger.info(f"Marked {len(to_skip) - len(failed_nodes)} dependent nodes as skipped")
```

### 5.4 DAG Planner (LLM-based)

```python
# app/agent/dag_planner.py
import json
import logging
from typing import Optional
from openai import AsyncAzureOpenAI
from app.agent.dag import ExecutionDAG, DAGNode

logger = logging.getLogger(__name__)

PLANNING_PROMPT = """
You are a task planning assistant. Given a user query, create an execution plan as a Directed Acyclic Graph (DAG).

Available tools:
- grep(pattern, path, recursive=true, ignore_case=false): Search for pattern in files
- find(path, name_pattern, type="f"): Find files by name pattern
- cat(path): Read file contents
- head(path, lines=10): Read first N lines
- tail(path, lines=10): Read last N lines
- ls(path): List directory contents
- wc(path): Count lines/words/chars in file

Rules:
1. Each node must have: id, tool, args
2. Add "depends_on" field if a node needs another node's output
3. Use variable syntax "$node_id.output" to reference another node's result
4. Minimize LLM calls - create a complete plan upfront
5. Maximize parallelization - only add dependencies when truly needed

Return JSON format:
{
  "nodes": [
    {"id": "step1", "tool": "find", "args": {"path": ".", "name_pattern": "*.py"}},
    {"id": "step2", "tool": "grep", "args": {"pattern": "TODO", "path": "$step1.output"}, "depends_on": ["step1"]}
  ]
}

User query: {query}

Return ONLY the JSON, no explanation.
"""

class DAGPlanner:
    """
    Generate execution DAGs from user queries using LLM.

    This planner uses the LLM to understand the user's intent and create
    an optimal execution plan as a DAG. The DAG can then be executed
    by DAGExecutor with automatic parallelization.
    """

    def __init__(self, client: AsyncAzureOpenAI, deployment_name: str):
        self.client = client
        self.deployment_name = deployment_name

    async def plan(self, query: str) -> Optional[ExecutionDAG]:
        """
        Generate a DAG execution plan for the given query.

        Args:
            query: User's query/request

        Returns:
            ExecutionDAG if successful, None if planning fails
        """
        try:
            # Ask LLM to create a plan
            response = await self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "You are a task planning expert."},
                    {"role": "user", "content": PLANNING_PROMPT.format(query=query)}
                ],
                temperature=0.1,  # Low temperature for consistent planning
            )

            plan_json = response.choices[0].message.content
            logger.debug(f"LLM generated plan: {plan_json}")

            # Parse JSON
            plan_data = json.loads(plan_json)

            # Convert to ExecutionDAG
            dag = self._build_dag_from_json(plan_data)

            # Validate
            is_valid, error = dag.validate()
            if not is_valid:
                logger.error(f"Invalid DAG generated: {error}")
                return None

            logger.info(f"Successfully generated DAG with {len(dag.nodes)} nodes")
            return dag

        except Exception as e:
            logger.exception(f"Error in DAG planning: {e}")
            return None

    def _build_dag_from_json(self, plan_data: dict) -> ExecutionDAG:
        """Convert JSON plan to ExecutionDAG."""
        dag = ExecutionDAG()

        for node_data in plan_data["nodes"]:
            node = DAGNode(
                id=node_data["id"],
                tool_name=node_data["tool"],
                arguments=node_data["args"],
                depends_on=node_data.get("depends_on", []),
            )
            dag.add_node(node)

        return dag
```

### 5.5 Integration v·ªõi FilesystemAgent

```python
# app/agent/filesystem_agent.py (modified)

class FilesystemAgent:
    def __init__(
        self,
        # ... existing params ...
        use_dag_planner: bool = False,  # NEW: Enable DAG-based execution
        dag_planner: Optional[DAGPlanner] = None,  # NEW
        dag_executor: Optional[DAGExecutor] = None,  # NEW
    ):
        # ... existing code ...
        self.use_dag_planner = use_dag_planner
        self.dag_planner = dag_planner
        self.dag_executor = dag_executor

    async def chat(
        self,
        user_message: str,
        history: Optional[list[dict]] = None,
    ) -> AgentResponse:
        """
        Process a user message and return a response.

        If use_dag_planner=True:
            1. Generate DAG plan from query
            2. Execute DAG with optimal parallelization
            3. Return results

        Otherwise: Use existing iterative approach
        """

        # NEW: DAG-based execution path
        if self.use_dag_planner and self.dag_planner and self.dag_executor:
            return await self._chat_with_dag(user_message, history)

        # Existing iterative execution path
        return await self._chat_iterative(user_message, history)

    async def _chat_with_dag(
        self,
        user_message: str,
        history: Optional[list[dict]] = None,
    ) -> AgentResponse:
        """
        DAG-based execution flow.

        Advantages:
        - Fewer LLM calls (1 planning call vs N iterations)
        - Better parallelization (layer-based vs simple parallel)
        - Clearer execution flow (visualizable DAG)
        """
        logger.info("Using DAG-based execution")

        # Step 1: Generate DAG plan
        dag = await self.dag_planner.plan(user_message)

        if dag is None:
            logger.warning("DAG planning failed, falling back to iterative approach")
            return await self._chat_iterative(user_message, history)

        # Step 2: Log execution plan
        plan = dag.get_execution_plan()
        logger.info(f"Execution plan: {plan['total_layers']} layers, "
                   f"max {plan['max_parallelism']} parallel operations")
        logger.debug(f"DAG visualization:\n{dag.visualize()}")

        # Step 3: Execute DAG
        results = await self.dag_executor.execute(dag)

        # Step 4: Synthesize final response with LLM
        synthesis_prompt = self._build_synthesis_prompt(user_message, dag, results)
        final_response = await self.client.chat.completions.create(
            model=self.deployment_name,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": synthesis_prompt},
            ],
        )

        # Step 5: Format response
        tool_calls = [
            ToolCall(
                id=node_id,
                name=dag.nodes[node_id].tool_name,
                arguments=dag.nodes[node_id].arguments,
            )
            for node_id in dag.nodes
        ]

        tool_results = [
            {
                "tool_call_id": node_id,
                "tool_name": dag.nodes[node_id].tool_name,
                "result": result.to_dict() if result else None,
            }
            for node_id, result in results.items()
        ]

        return AgentResponse(
            message=final_response.choices[0].message.content,
            tool_calls=tool_calls,
            tool_results=tool_results,
        )

    def _build_synthesis_prompt(
        self,
        user_message: str,
        dag: ExecutionDAG,
        results: Dict[str, ExecutionResult],
    ) -> str:
        """Build prompt for final response synthesis."""

        # Format results for LLM
        results_text = []
        for node_id, result in results.items():
            node = dag.nodes[node_id]
            status = "‚úì" if result.success else "‚úó"
            output = result.stdout[:500] if result.success else result.stderr[:500]
            results_text.append(
                f"{status} {node.tool_name}({node.arguments})\n"
                f"  Output: {output}"
            )

        return f"""
User asked: {user_message}

I executed the following operations:
{chr(10).join(results_text)}

Please synthesize these results into a helpful, natural language response to the user.
"""

    async def _chat_iterative(self, user_message, history):
        """Existing iterative approach (unchanged)."""
        # ... existing code from current chat() method ...
        pass
```

---

## 6. Roadmap Tri·ªÉn Khai

### 6.1 Phase 1: Foundation (Week 1-2)

#### Deliverables:
- [ ] DAG data structures (`app/agent/dag.py`)
- [ ] Basic DAGExecutor v·ªõi topological sort
- [ ] Unit tests cho DAG validation v√† execution
- [ ] Documentation

#### Tasks:
```python
# Week 1
- Implement DAGNode, ExecutionDAG classes
- Write validate() method v·ªõi cycle detection
- Write topological_sort() method
- Add visualization utilities

# Week 2
- Implement DAGExecutor.execute()
- Implement layer-by-layer parallel execution
- Handle error propagation
- Write comprehensive tests
```

#### Success Metrics:
- [ ] All tests pass (>90% coverage)
- [ ] Can execute simple 3-node DAG
- [ ] Cycle detection works correctly
- [ ] Visualization outputs readable ASCII art

### 6.2 Phase 2: LLM Integration (Week 3-4)

#### Deliverables:
- [ ] DAGPlanner v·ªõi LLM-based planning
- [ ] Integration v·ªõi FilesystemAgent
- [ ] A/B testing framework (DAG vs iterative)
- [ ] Performance benchmarks

#### Tasks:
```python
# Week 3
- Implement DAGPlanner.plan()
- Design planning prompt template
- Add JSON parsing v√† error handling
- Test with various query types

# Week 4
- Integrate into FilesystemAgent.chat()
- Add use_dag_planner flag
- Implement fallback mechanism
- Write integration tests
```

#### Success Metrics:
- [ ] DAG planner success rate >80%
- [ ] Performance improvement 2-3x for complex queries
- [ ] Fewer LLM calls (30-50% reduction)
- [ ] User-facing API unchanged (backward compatible)

### 6.3 Phase 3: Cache Integration (Week 5-6)

#### Deliverables:
- [ ] DAG-aware cache invalidation
- [ ] Cache dependency tracking
- [ ] Persistent DAG cache
- [ ] Cache visualization tools

#### Tasks:
```python
# Week 5
- Extend CacheManager v·ªõi DAG support
- Implement dependency graph tracking
- Add invalidate_dag() method
- Test cache correctness

# Week 6
- Integrate v·ªõi FileStateTracker
- Add cache key generation based on DAG structure
- Implement intelligent pre-fetching
- Performance optimization
```

#### Success Metrics:
- [ ] Cache hit rate improvement 20-30%
- [ ] Precise invalidation (no over-invalidation)
- [ ] Dependency graph correctly maintained
- [ ] Cache performance tests pass

### 6.4 Phase 4: Advanced Features (Week 7-8)

#### Deliverables:
- [ ] Dynamic DAG restructuring
- [ ] Conditional branching
- [ ] Error recovery strategies
- [ ] DAG visualization UI

#### Tasks:
```python
# Week 7
- Implement conditional nodes (if-then-else)
- Add dynamic node insertion
- Implement retry mechanism
- Add circuit breaker pattern

# Week 8
- Build web UI for DAG visualization
- Add real-time execution monitoring
- Implement DAG optimization passes
- Create example notebooks
```

#### Success Metrics:
- [ ] Support conditional workflows
- [ ] Error recovery rate >90%
- [ ] UI displays real-time DAG execution
- [ ] Complete documentation v√† examples

---

## 7. So S√°nh V·ªõi LangGraph

### 7.1 Feature Comparison

| Feature | LangGraph | Proposed DAG System | Notes |
|---------|-----------|---------------------|-------|
| **DAG Support** | ‚úÖ Native | ‚úÖ Native | Both support DAG workflows |
| **Cycles** | ‚úÖ Supported | ‚ùå Pure DAG | LangGraph allows cycles for agent loops |
| **Parallel Execution** | ‚úÖ Built-in | ‚úÖ Built-in | Both optimize parallelization |
| **LLM Planning** | ‚ùå Manual | ‚úÖ Dynamic | Our system can generate DAGs via LLM |
| **Tool Integration** | ‚úÖ Extensive | ‚úÖ Custom | LangGraph has more pre-built tools |
| **Cache System** | ‚ö†Ô∏è Basic | ‚úÖ Advanced | Our multi-tier cache is more sophisticated |
| **Filesystem Focus** | ‚ùå General | ‚úÖ Specialized | Optimized for file operations |
| **Security Sandbox** | ‚ùå None | ‚úÖ Built-in | Path confinement, whitelisting |
| **Learning Curve** | ‚ö†Ô∏è Steep | ‚úÖ Moderate | Simpler API for our use case |

### 7.2 When to Use Each

#### Use LangGraph if:
- Need complex multi-agent coordination
- Require cycles (e.g., agent self-reflection loops)
- Want extensive pre-built integrations
- Building general-purpose agent system

#### Use Proposed DAG System if:
- Focus on filesystem/bash operations
- Need high security (sandboxing)
- Want sophisticated caching
- Prefer simple, focused API
- Already using this codebase

### 7.3 Hybrid Approach

C√≥ th·ªÉ k·∫øt h·ª£p c·∫£ hai:

```python
# Use LangGraph for high-level workflow
from langgraph.graph import Graph

workflow = Graph()
workflow.add_node("research", research_agent)  # Uses LangGraph
workflow.add_node("filesystem_analysis", filesystem_dag_agent)  # Uses our DAG
workflow.add_node("report", report_agent)  # Uses LangGraph

workflow.add_edge("research", "filesystem_analysis")
workflow.add_edge("filesystem_analysis", "report")
```

---

## 8. K·∫øt Lu·∫≠n

### 8.1 T√≥m T·∫Øt L·ª£i √çch

#### Performance
- **2-3x faster** cho complex queries nh·ªù better parallelization
- **30-50% fewer LLM calls** nh·ªù upfront planning
- **Cache hit rate improvement** 20-30% v·ªõi DAG-aware invalidation

#### Developer Experience
- **Clear execution flow** - C√≥ th·ªÉ visualize v√† debug d·ªÖ d√†ng
- **Predictable behavior** - DAG structure r√µ r√†ng h∆°n N iterations
- **Better testability** - Test t·ª´ng node ƒë·ªôc l·∫≠p

#### User Experience
- **Faster responses** - Reduced latency
- **More reliable** - Better error handling
- **Explainable** - User c√≥ th·ªÉ th·∫•y execution plan

### 8.2 R·ªßi Ro v√† Mitigations

| R·ªßi Ro | Impact | Mitigation |
|--------|--------|------------|
| LLM planning sai | High | Fallback to iterative approach |
| Over-engineering | Medium | Start simple, iterate based on metrics |
| Breaking changes | Low | Feature flag, backward compatibility |
| Increased complexity | Medium | Good documentation, examples |

### 8.3 Next Steps

1. **Review v√† Feedback** (Week 0)
   - Team review document n√†y
   - Gather feedback v√† requirements
   - Prioritize features

2. **Prototype** (Week 1-2)
   - Build Phase 1 foundation
   - Test v·ªõi real queries
   - Measure performance gains

3. **Iterate** (Week 3-8)
   - Follow roadmap phases
   - Continuous testing v√† optimization
   - Collect user feedback

4. **Production** (Week 9+)
   - Enable by default (sau khi validated)
   - Monitor metrics
   - Continuous improvement

---

## Appendix A: Code Examples

### Example 1: Simple DAG Execution

```python
from app.agent.dag import ExecutionDAG, DAGNode
from app.agent.dag_executor import DAGExecutor

# Create DAG
dag = ExecutionDAG()

# Add nodes
dag.add_node(DAGNode(
    id="find_python",
    tool_name="find",
    arguments={"path": ".", "name_pattern": "*.py"}
))

dag.add_node(DAGNode(
    id="count_lines",
    tool_name="wc",
    arguments={"files": "$find_python.output"},
    depends_on=["find_python"]
))

# Execute
executor = DAGExecutor(orchestrator)
results = await executor.execute(dag)

print(results["count_lines"].stdout)
```

### Example 2: Complex Multi-Branch DAG

```python
# Query: "Find TODO and FIXME comments, categorize by priority"

dag = ExecutionDAG()

# Branch 1: TODO comments
dag.add_node(DAGNode(
    id="find_todos",
    tool_name="grep",
    arguments={"pattern": "TODO", "path": ".", "recursive": True}
))

# Branch 2: FIXME comments
dag.add_node(DAGNode(
    id="find_fixmes",
    tool_name="grep",
    arguments={"pattern": "FIXME", "path": ".", "recursive": True}
))

# Merge branches
dag.add_node(DAGNode(
    id="categorize",
    tool_name="process",  # Custom tool
    arguments={
        "todos": "$find_todos.output",
        "fixmes": "$find_fixmes.output"
    },
    depends_on=["find_todos", "find_fixmes"]
))

# Execution flow:
#      find_todos  ‚îÄ‚îê
#                   ‚îú‚îÄ‚Üí categorize
#     find_fixmes  ‚îÄ‚îò
```

---

## Appendix B: References

### Academic Papers
- "Directed Acyclic Graphs: The Backbone of Modern Multi-Agent AI" (2025)
- "Agentic AI workflows in Directed Acyclic Graphs (DAGs)" (2025)

### Frameworks
- [LangGraph](https://github.com/langchain-ai/langgraph) - DAG-based agent framework from LangChain
- [DAGent](https://github.com/dagent/dagent) - Open-source DAG AI agent library
- [Apache Airflow](https://airflow.apache.org/) - DAG-based workflow orchestration

### Blog Posts
- [Vercel: How to Build Agents with Filesystems and Bash](https://vercel.com/blog/how-to-build-agents-with-filesystems-and-bash)
- [Getting Started with LangGraph](https://doggydish.com/getting-started-with-langgraph-build-your-first-dag-based-agent-flow/)

### Internal Documentation
- [CLAUDE.md](../CLAUDE.md) - Project overview
- [CACHE_INTEGRATION_GUIDE.md](./CACHE_INTEGRATION_GUIDE.md) - Cache system documentation

---

**Document Version**: 1.0
**Last Updated**: 2026-01-23
**Author**: AI Analysis based on codebase review
**Status**: Proposal - Awaiting Review
